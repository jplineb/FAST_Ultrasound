# -*- coding: utf-8 -*-
"""Watson FAST Model Calibration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hQa_GxaK5JyLOd0huCnJ6JDjwOq073Dw
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.metrics import confusion_matrix


calib_frac = 0.33

"""## Load and prepare the data"""

df = pd.read_csv("FASTDataFrameNice.csv")
df.head()

# pull off target and variable columns
y = df.actual_qual.values
X = df.iloc[:, 3:8].values
X[:5]

# get sample weights to balance quality levels
qual_counts = df.groupby("actual_qual")['actual_qual'].agg('count').values

weights =  (234/5)/qual_counts
weights = weights**2
print(weights)
print(qual_counts/ sum(qual_counts) * weights)

def get_weight(qual):
  wt = weights[qual-1]
  return wt

get_weight(3)

y_weights = df.actual_qual.apply(get_weight)
print(y_weights)

df['Weights'] = y_weights
df.head()

figuresfromallruns = []
####################### FOR LOOP ####################################
for x in range(0, 1000):
    # train/test split
    # X_train, X_test, y_train, y_test, y_weights_train, _ = train_test_split(X, y, y_weights, test_size=1-calib_frac, stratify=y)
    df_train, df_test = train_test_split(df, test_size=1-calib_frac, stratify=df.actual_qual)
    df_test.head()
    
    # Split the train and test data set
    
    ### Test ###
    y_dftest = df_test.actual_qual.values # is array
    x_dftest = df_test.iloc[:, 3:8].values # is array
    
    
    ### Train ###
    y_dftrain = df_train.actual_qual.values # is array
    x_dftrain = df_train.iloc[:, 3:8].values # is array
    y_dftrain_weights = df_train.Weights.values # is array
    x_dftrain[1:3]

    
    """## Train the calibration model"""
    
    estim = LinearRegression(fit_intercept= False)
    estim.fit(x_dftrain, y_dftrain, sample_weight=y_dftrain_weights)
    
    # evaluate the mode:
    
    # look at fit coefs
    print(estim.coef_)
    # -> array([1.76293662, 2.13542102, 2.92516357, 3.10297302, 3.49912872])
    
    """## Get final scores"""
    
    # evaluate model on training set
    y_test_pred = estim.predict(x_dftest)
    
    np.sqrt(mean_squared_error(y_dftest, y_test_pred))
    
    mean_absolute_error(y_dftest, y_test_pred)
    
    df_eval = pd.DataFrame(x_dftest)
    df_eval['y_test'] = y_dftest
    df_eval['y_test_pred'] = y_test_pred
    df_eval.head()
    
    df_fulltest = df_test.reset_index()
    df_fulltest['calibration_predict'] = y_test_pred
    df_fulltest.head()
    
    
    
    # confusion matrix
    CalQual = df_fulltest["calibration_predict"].values.tolist()
    i = 0
    for x in CalQual:
      if x<1:
        CalQual[i] = 1
      elif x> 5:
        CalQual[i] = 5
      else:
        CalQual[i] = round(x)
      i = i + 1
      

      
    TruQual = df_fulltest['actual_qual'].values.tolist()
    
    cm = confusion_matrix(TruQual, CalQual)
    figuresfromallruns.append(cm)
################################################################
    
totalmatrix = np.zeros((5,5))
for x in figuresfromallruns:
    totalmatrix = totalmatrix + x
Averagematrix = totalmatrix/(len(figuresfromallruns))
Averagematrix = Averagematrix.astype(int)


    ### Confusion Matrix Function ###
def plot_confusion_matrix_havematrix(cm, classes, normalize = False, title = None, cmap=plt.cm.Blues):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return fig
    #################################
    
    
Averagematrixfig = plot_confusion_matrix_havematrix(Averagematrix,
                                                    classes = ["Quality 1", "Quality 2", "Quality 3", "Quality 4", "Quality 5"],
                                                    title = "Average Calibrated Predictions")
plt.savefig("confusion_matrix_WOnorm.png") 
AverageNormalizedConfusionMatrix = plot_confusion_matrix_havematrix(Averagematrix,
                                                    classes = ["Quality 1", "Quality 2", "Quality 3", "Quality 4", "Quality 5"],
                                                    title = "Average Calibrated Predictions Normalized",
                                                    normalize = True)  
plt.savefig("confusion_matrix_norm.png")
